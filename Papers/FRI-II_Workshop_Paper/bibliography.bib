@inproceedings{zellers2018scenegraphs,
  title={Neural Motifs: Scene Graph Parsing with Global Context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle = "Conference on Computer Vision and Pattern Recognition",  
  year={2018}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{lueddecke22_cvpr,
    author    = {L\"uddecke, Timo and Ecker, Alexander},
    title     = {Image Segmentation Using Text and Image Prompts},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7086-7096}
}

@article{Krishna_Zhu_Groth_Johnson_Hata_Kravitz_Chen_Kalantidis_Li_Shamma_et_al._2017, 
    title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
    volume={123}, 
    DOI={10.1007/s11263-016-0981-7}, 
    number={1}, 
    journal={International Journal of Computer Vision}, 
    author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and et al.}, 
    year={2017}, 
    pages={32–73}
} 

@misc{yang2018graph,
      title={Graph R-CNN for Scene Graph Generation}, 
      author={Jianwei Yang and Jiasen Lu and Stefan Lee and Dhruv Batra and Devi Parikh},
      year={2018},
      eprint={1808.00191},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{tang2020sggcode,
    title = {A Scene Graph Generation Codebase in PyTorch},
    author = {Tang, Kaihua},
    year = {2020},
    note = {\url{https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch}},
}

@inproceedings{tang2018learning,
    title={Learning to Compose Dynamic Tree Structures for Visual Contexts},
    author={Tang, Kaihua and Zhang, Hanwang and Wu, Baoyuan and Luo, Wenhan and Liu, Wei},
    booktitle= "Conference on Computer Vision and Pattern Recognition",
    year={2019}
}

@inproceedings{tang2020unbiased,
    title={Unbiased Scene Graph Generation from Biased Training},
    author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
    booktitle= "Conference on Computer Vision and Pattern Recognition",
    year={2020}
}

@article{robinson2023vision,
    title = {Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review},
    author = {Robinson, Nicole and Tidd, Brendan and Campbell, Dylan and Kuli\'{c}, Dana and Corke, Peter},
    year = {2023},
    issue_date = {March 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {12},
    number = {1},
    url = {https://doi.org/10.1145/3570731},
    doi = {10.1145/3570731},
    abstract = {Robotic vision, otherwise known as computer vision for robots, is a critical process for robots to collect and interpret detailed information related to human actions, goals, and preferences, enabling robots to provide more useful services to people. This survey and systematic review presents a comprehensive analysis on robotic vision in human-robot interaction and collaboration (HRI/C) over the past 10&nbsp;years. From a detailed search of 3,850 articles, systematic extraction and evaluation was used to identify and explore 310 papers in depth. These papers described robots with some level of autonomy using robotic vision for locomotion, manipulation, and/or visual communication to collaborate or interact with people. This article provides an in-depth analysis of current trends, common domains, methods and procedures, technical processes, datasets and models, experimental testing, sample populations, performance metrics, and future challenges. Robotic vision was often used in action and gesture recognition, robot movement in human spaces, object handover and collaborative actions, social communication, and learning from demonstration. Few high-impact and novel techniques from the computer vision field had been translated into HRI/C. Overall, notable advancements have been made on how to develop and deploy robots to assist people.},
    journal = {J. Hum.-Robot Interact.},
    month = {feb},
    articleno = {12},
    numpages = {66},
    keywords = {human-robot interaction, computer vision, Robotic vision, social communication, learning from demonstration, collaborative actions, object handover, robot movement in human spaces, gesture recognition}
}

@misc{latestinML, 
    url={https://paperswithcode.com/sota/scene-graph-generation-on-visual-genome}, 
    journal={The latest in Machine Learning}
} 

@inproceedings{azenkot2016enabling,
    author = {Azenkot, Shiri and Feng, Catherine and Cakmak, Maya},
    title = {Enabling Building Service Robots to Guide Blind People: A Participatory Design Approach},
    year = {2016},
    isbn = {9781467383707},
    publisher = {IEEE Press},
    abstract = {Building service robots-robots that perform various services in buildings-are becoming more common in large buildings such as hotels and stores. We aim to leverage such robots to serve as guides for blind people. In this paper, we sought to design specifications that detail how a building service robot could interact with and guide a blind person through a building in an effective and socially acceptable way. We conducted participatory design sessions with three designers and five non-designers. Two of the designers and all of the non-designers had a vision disability. Primary features of the design include allowing the user to (1) summon the robot after entering the building, (2) choose from three modes of assistance (Sighted Guide, Escort, and Information Kiosk), and (3) receive information about the building's layout from the robot. We conclude with a discussion of themes and a reflection about our design process that can benefit robot design for blind people in general.},
    booktitle = {The Eleventh ACM/IEEE International Conference on Human Robot Interaction},
    pages = {3–10},
    numpages = {8},
    keywords = {accessibility, blind, robots, participatory design.},
    location = {Christchurch, New Zealand},
    series = {HRI '16}
}